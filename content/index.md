---
title: Coby Kassner
cssclasses:
  - hide-title
  - hide-meta
---


<script src="https://unpkg.com/feather-icons"></script>
<script>
  function initializeIcons() {
    feather.replace();
  }

  // Initialize on first load
  document.addEventListener('DOMContentLoaded', initializeIcons);
  
  // Re-initialize on page navigation
  document.addEventListener('nav', initializeIcons);
</script>

<div class="social-icons">
  <a href="https://cobylk.io/curriculum_vitae.pdf" target="_blank" class="tooltip" data-tooltip="Curriculum Vitae" data-cursor="pointer">
    <i data-feather="file-text"></i>
  </a>
  <a href="https://github.com/cobylk" target="_blank" class="tooltip" data-tooltip="GitHub Profile" data-cursor="pointer">
    <i data-feather="github"></i>
  </a>
  <a href="https://www.linkedin.com/in/cobylk" target="_blank" class="tooltip" data-tooltip="LinkedIn Profile" data-cursor="pointer">
    <i data-feather="linkedin"></i>
  </a>
  <a href="mailto:kassner@cobylk.io" class="tooltip" data-tooltip="Email" data-cursor="pointer">
    <i data-feather="mail"></i>
  </a>
</div>


### Hey there!
My name is Coby, and I'm a student researcher with interest in mechanistic interpretability and a broader interest in most things related to technical AI safety and alignment. In my free time, I love composing and [performing classical music](tooltip:My primary instrument is the piano, though I played saxophone somewhat well and have unsuccessfully tried to learn the violin on several ocassions.), hiking, and skiing. I'm currently doing my [undergrad](tooltip:Class of 2029) at Yale.

See my [research](Research/index) or [personal writing](Personal/index).

> [!danger]  Construction Zone!
> This website is very much so in progress. I plan to write much more, flesh things out, and fix things in the near future. Forgive me if you encounter anything that is buggy, incomplete, or unprofessional.

***
### News
- *August 2025* — I completed the Yale Effective Altruism introductory fellowship.
- *May 2025* — I finished SPAR and presented our findings (so far) at the poster session. We are continuing to work on the project beyond the program
- *February 2025* — I was accepted to the [SPAR](https://sparai.org) program under Dr. Ronak Mehta to research models that are inherently interpretable.
- *November 2024* — Our [activation steering data extraction](Research/caa_data_extraction) project placed 7th in the [LLM Privacy Contest](https://llm-pc.github.io/) at NeurIPS