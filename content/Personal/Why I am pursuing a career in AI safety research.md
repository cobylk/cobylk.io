---
title: Why I am pursuing a career in AI safety research
draft: true
---
A child is dawdling along a street when a man—visibly intoxicated on rationalist literature—approaches. He is a doctor. He studies the child and begins defending an intricate thesis. Startled but curious, the child listens. The man dissects his priors, enumerates biases, outlines the Bayesian update process (of course), and continues muttering about Boltzmann brains, meta-contrarian equilibria, anthropic shadow bias, counterfactual mugging, coherent extrapolated volition, the Löbian obstacle, acausal trade with Omega, and whether utility functions over Tegmark Level IV multiverses require renormalization. Eventually, his obviously coherent reasoning and greedy adherence to Occam’s Razor *compel* him to conclude— the child will die of cancer within a few years. 

 The child absorbs this and after an uncomfortable silence, proclaims: “I will become a cancer researcher and cure it myself!”

In this blog, I will (hopefully) try to show that this child is not me.
### AI timelines and outcomes
The question of the usefulness of a career dedicated to AI safety is modulated by a few questions surrounding whether the continued development of AI will actually create risks (existential or otherwise) that will impact many people, and when such risks may come into play.
#### Feasibility of vast superintelligence

(I need to define superintelligence somewhere)

Humans evolved in an environment with many opposing pressures. Every additional cubic centimeter of brain volume increases the difficulty and risk of childbirth; the human birth canal cannot become arbitrarily wide without stifling mobility and making impractical our bipedalism. Every additional molecule of glucose consumed by our brain is one that cannot not be put to use elsewhere. Larger, more complicated brains increase the length of childhood, increase dependence, and thus heighten mortality before reproductive age. Larger brains have more genetic "moving parts" that can be disrupted, creating higher rates of developmental disorders. In general, humans were only selected for intelligence insofar as it had instrumental value for reproduction. Evolution, as a mechanism for producing intelligence, does not seem particularly optimal.

Indeed, our intelligence and agency have enabled us to *beat* evolution, again and again, at the task of design in service of some objective. Our designs are not constrained by confounding evolutionary pressures or the requirement to implement designs as multicellular organisms encoded with DNA, nor are they hampered by the fact that evolution simply does not optimize for the characteristics that we can optimize for. The billions of years of selection that yielded aerodynamic falcon bodies that dive faster than any other animal did not yield the scramjets we designed that propel the X-43 at [ten times](tooltip: to be precise, only Mach 9.6 :() the speed of sound. We also designed Saturn V rocket engines that carry a 310,000 pound payload beyond Earth's atmosphere by generating the force of 160 million horses, neutrino detectors that register uncharged, nigh massless particles passing through eight hundred miles more rock than any biological detector can do anything with, and transistors that switch states ten million times faster than human neurons fire. Now, in the 21st century, we have taken seriously to the task of recreating the characteristic that has enabled all of our innovations.

The biological implementation of intelligence is a single point in a vast design space. Again, because so many conflicting pressures were involved, human brains are probably very suboptimal intelligences compared to an alternative engineered with fewer constraints and with the sole objective of intelligence. We have no current [strong evidence](tooltip: besides that, well, we haven't figured it out yet) that creating (superior) non-biological implementations of intelligence is a particularly intractable problem. We do have [evidence](tooltip: the subject of the following sections of this article) that we have already created systems with many aspects of biological intelligence, and that these may lead to superintelligences.

That human brains exist and can be created is itself evidence for the feasibility of superintelligence—if no alternative approaches pan out, barring specific [defeat conditions](tooltip: e.g., incorporeal sources of intelligence, non-computable neural microphysics, or indispensable quantum effects at cognitive scales, for which we have no evidence), it could be [possible](tooltip: we already have an entire fruit fly connectome and a rudimentary simulation of its activity; the researchers "fully expect" to be able to extend their work to more complex connectomes, including the human brain; it seems plausible that this could lead to a full human brain simulation within the next few hundred years; this is a weak standard of evidence) to simulate an ensemble of human brains collaborating at a faster than real-time pace, and this would yield something that can do every intellectual task that a human could, but quicker and better. This is a superintelligence.

https://news.berkeley.edu/2024/10/02/researchers-simulate-an-entire-fly-brain-on-a-laptop-is-a-human-brain-next/
#### Capability of LLMs for novel research
 - Surface level vs. deep generality
 - Stochastic parrots
		- General intelligence is/isn't required for research automation
			- https://www.alignmentforum.org/posts/k38sJNLk7YbJA72ST/llm-generality-is-a-timeline-crux
			- https://www.lesswrong.com/posts/wN4oWB4xhiiHJF9bS/llms-look-increasingly-like-general-reasoners
			- https://www.lesswrong.com/posts/RSqfcyAW9ZkveGQ5u/numberwang-llms-doing-autonomous-research-and-a-call-for-1
		- *Current LLMs* can plausibly contribute to research
			- Scaffolds
				- https://arxiv.org/abs/2506.13131
			- No, "brute-forcing" research isn't the only path
				- https://arxiv.org/abs/2406.14546
				- https://arxiv.org/abs/2408.09503
			- Anecdotes, other weak evidence
				- https://arxiv.org/abs/2404.04326
				- https://arxiv.org/abs/2409.04109
				- https://www.lesswrong.com/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights
				- CLR/ILR transformers
			- These capabilities will continue to scale, and OpenAI etc. are specifically pursuing a singularity by focusing on AI research capability in their models.
	- Scenario timings
		- Scaling
		- Plateau scenarios
		- Alternatives to scaling LMs; alternative timelines
	- **An enumeration of scenarios including timing and capability, weighted by probability**
- Outcomes by capability
	- Takeover
	- Value lock-in
- The meaning of life
	- I am not a utilitarian
	- 