---
title: Why I am pursuing a career in AI safety research
draft: true
---
A child is dawdling along a street when a man—visibly intoxicated on rationalist literature—approaches. He is a doctor. He studies the child and begins defending an intricate thesis. Startled but curious, the child listens. The man dissects his priors, enumerates biases, outlines the Bayesian update process (of course), and continues muttering about Boltzmann brains, meta-contrarian equilibria, anthropic shadow bias, counterfactual mugging, coherent extrapolated volition, the Löbian obstacle, acausal trade with Omega, and whether utility functions over Tegmark Level IV multiverses require renormalization. Eventually, his obviously coherent reasoning and greedy adherence to Occam’s Razor *compel* him to conclude— the child will die of cancer within a few years. 

 The child absorbs this and after an uncomfortable silence, proclaims: “I will become a cancer researcher and cure it myself!”

In this blog, I will (hopefully) try to show that this child is not me.
### AI timelines and outcomes
The question of the usefulness of a career dedicated to AI safety is modulated by a few questions surrounding whether the continued development of AI will actually create risks (existential or otherwise) that will impact many people, and when such risks may come into play.
#### Feasibility of vast superintelligence

Humans evolved in an environment with many opposing pressures. Every additional cubic centimeter of brain volume increases the difficulty and risk of childbirth; the human birth canal cannot become arbitrarily wide without stifling mobility and making impractical our bipedalism. Every additional molecule of glucose consumed by our brain is one that cannot not be put to use elsewhere. Larger, more complicated brains increase the length of childhood, increase dependence, and thus heighten mortality before reproductive age. Larger brains have more genetic "moving parts" that can be disrupted, creating higher rates of developmental disorders. Despite this, over the course of the few million years after we split from chimpanzees, some threshold was crossed that enabled us to create civilization, and it is difficult to justify why such a threshold would not be surpassed much earlier if all the other pressures and constraints did not exist. 

Indeed, our intelligence and agency have enabled use to *beat* evolution, again and again, at the task of design in service of some objective. The billions of years of selection that yielded aerodynamic falcon bodies that dive faster than any other animal did not yield the scramjets that propel the X-43 at nearly ten times the speed of sound. Human designs are not constrained by confounding evolutionary pressures or the requirement to implement designs as multicellular organisms encoded with DNA.
- Governments and militaries are superintelligences: many humans working together can create something like an emergent superintelligence.
- It seems plausible, at the very least, to create a superintelligence by simulating many human-level intelligences working together and at an extremely fast rate

#### Capability of LLMs for novel research
 - Surface level vs. deep generality
 - Stochastic parrots
		- General intelligence is/isn't required for research automation
			- https://www.alignmentforum.org/posts/k38sJNLk7YbJA72ST/llm-generality-is-a-timeline-crux
			- https://www.lesswrong.com/posts/wN4oWB4xhiiHJF9bS/llms-look-increasingly-like-general-reasoners
			- https://www.lesswrong.com/posts/RSqfcyAW9ZkveGQ5u/numberwang-llms-doing-autonomous-research-and-a-call-for-1
		- *Current LLMs* can plausibly contribute to research
			- Scaffolds
				- https://arxiv.org/abs/2506.13131
			- No, "brute-forcing" research isn't the only path
				- https://arxiv.org/abs/2406.14546
				- https://arxiv.org/abs/2408.09503
			- Anecdotes, other weak evidence
				- https://arxiv.org/abs/2404.04326
				- https://arxiv.org/abs/2409.04109
				- https://www.lesswrong.com/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights
				- CLR/ILR transformers
			- These capabilities will continue to scale, and OpenAI etc. are specifically pursuing a singularity by focusing on AI research capability in their models.
	- Scenario timings
		- Scaling
		- Plateau scenarios
		- Alternatives to scaling LMs; alternative timelines
	- **An enumeration of scenarios including timing and capability, weighted by probability**
- Outcomes by capability
	- Takeover
	- Value lock-in
- The meaning of life
	- I am not a utilitarian
	- 